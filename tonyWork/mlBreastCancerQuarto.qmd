---
title: "mlBreastCancerQuarto"
author: "Tony Song"
format: html
editor: visual
---

## Plan

-   Import Dataset

-   Check Data

-   Process Data

-   Split into training and testing

-   Evaluate Models

-   Feature Scaling (PCA)

-   Compress Data

-   Hyper Parameter Tuning

-   Evaluating Models

-   Cross Validation

-   Select Best Model

-   Shiny App

Import Libraries

```{r}
library(psych)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(shiny)
library(shinythemes)
library(tibble)
library(vroom)
library(fastDummies)
library(caTools)
library(class)
library(naivebayes)
library(paletteer)
library(randomForest)
library(shinyjs)
library(ggcorrplot)
library(utils)
library(pROC)
library(class)
library(reshape2)
library(gridExtra)
library(utils)
library(FNN)
library(mlbench)
library(caret)
library(ggbiplot)
library(devtools)
library(varImp)
library(e1071)
library(adabag)
library(rpart)
library(gbm)
library(nnet)
library(glmnet)
library(gganimate)
library(gifski)
library(png)  
```

### Import Dataset

```{r}
setwd("C:/Users/Tony/OneDrive/Documents/tonyR/DSRP-2024-Khevna/tonyWork")
data <- read.csv("../data/breast_cancer_classification_data.csv")
```

### Clean Data

```{r}
data <- data[,c(-1,-33)]
```

```{r}
head(data)
```

```{r}
#ggplot(data,aes(x=radius_mean,y=radius_worst)) + geom_point() + geom_smooth(method="lm")
#tempmodel <- lm(radius_worst ~ radius_mean, data = data)
#slope <- coef(tempmodel)["radius_mean"]
#print(slope)
```

```{r}
correlation_matrix <- cor(data[,-1],use = "complete.obs")
```

```{r}
highlyCorrelated <- findCorrelation(cor(data[,-1]), cutoff=0.9,names=TRUE)
highlyCorrelated


```

```{r}
highlyCorrelated <- findCorrelation(cor(select(data,-c("diagnosis","concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean"))), cutoff=0.9,names=TRUE)
highlyCorrelated

```

```{r}
set.seed(42)
dataset <- data
split <- sample.split(dataset$diagnosis,SplitRatio=0.7)
training <- subset(dataset,split==TRUE)
testing <- subset(dataset,split==FALSE)
training$diagnosis <- as.factor(training$diagnosis)
testing$diagnosis <- as.factor(testing$diagnosis)
```

```{r}
trainingSubset <- training %>% select(-c("concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean"))
testingSubset <- testing %>% select(-c("concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean"))
```

```{r}
rf_model <- randomForest(x = training[,-1],
                         y = training$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testing[,-1])

actuals <- testing$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracyrf1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracyrf1*100, 2), "%")
```

```{r}
importance_scores <- importance(rf_model)
importance_df <- as.data.frame(importance_scores)
importance_df$Feature <- rownames(importance_df)
importance_sorted <- importance_df[order(-importance_df$MeanDecreaseGini), ]
print(importance_sorted)
varImpPlot(rf_model)
```

```{r}
trainingFeatures <- training %>% select(c("diagnosis","radius_worst","concave.points_worst","perimeter_worst","concave.points_mean","area_worst","radius_mean"))
testingFeatures <- testing %>% select(c("diagnosis","radius_worst","concave.points_worst","perimeter_worst","concave.points_mean","area_worst","radius_mean"))
```

```{r}
rf_model <- randomForest(x = trainingFeatures[,-1],
                         y = trainingFeatures$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testingFeatures[,-1])

actuals <- testingFeatures$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

```{r}
rf_model <- randomForest(x = trainingSubset[,-1],
                         y = trainingSubset$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testingSubset[,-1])

actuals <- testingSubset$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracyrf2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracyrf2*100, 2), "%")
```

```{r}
importance_scores <- importance(rf_model)
importance_df <- as.data.frame(importance_scores)
importance_df$Feature <- rownames(importance_df)
importance_sorted <- importance_df[order(-importance_df$MeanDecreaseGini), ]
print(importance_sorted)
varImpPlot(rf_model)
```

```{r}
trainingSubsetFeatures <- trainingSubset %>% select(c("diagnosis","concave.points_worst","radius_worst","concavity_worst","radius_se","compactness_mean"))
testingSubsetFeatures <- testingSubset %>% select(c("diagnosis","concave.points_worst","radius_worst","concavity_worst","radius_se","compactness_mean"))
```

```{r}
rf_model <- randomForest(x = trainingSubsetFeatures[,-1],
                         y = trainingSubsetFeatures$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testingSubsetFeatures[,-1])

actuals <- testingSubsetFeatures$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

```{r}
knn_model <- knn(train = training[,-1], test=testing[,-1],cl=training$diagnosis, k=5)

actuals <- testing$diagnosis

confusion_matrix <- table(Predicted = knn_model, Actual = actuals)

accuracyknn1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracyknn1*100, 2), "%")
```

```{r}
knn_model <- knn(train = trainingSubset[,-1], test=testingSubset[,-1],cl=training$diagnosis, k=5)

actuals <- testingSubset$diagnosis

confusion_matrix <- table(Predicted = knn_model, Actual = actuals)

accuracyknn2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracyknn2*100, 2), "%")
```

```{r}
p <- cor(select(data,-c("diagnosis","concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean")),use="complete.obs") >0.9
ggcorrplot(p)
```

SVM Model

```{r}
svm_model <- svm(diagnosis ~ ., data = training, kernel = "radial", cost = 1, scale = TRUE)

predictions <- predict(svm_model, testing)
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)

accuracysvm1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracysvm1*100, 2), "%")
```

```{r}
svm_model <- svm(diagnosis ~ ., data = trainingSubset, kernel = "radial", cost = 1, scale = TRUE)

predictions <- predict(svm_model, testingSubset)
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)

accuracysvm2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracysvm2*100, 2), "%")
```

Decision Tree Model

```{r}
dt_model <- rpart(diagnosis ~ ., data = training, method = "class")
predictions <- predict(dt_model, testing, type = "class")
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracydt1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracydt1*100, 2), "%")
```

```{r}
dt_model <- rpart(diagnosis ~ ., data = trainingSubset, method = "class")
predictions <- predict(dt_model, testingSubset, type = "class")
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracydt2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracydt2*100, 2), "%")
```

Naive Bayes Classifier

```{r}
nb_model <- naiveBayes(diagnosis ~ ., data = training)
predictions <- predict(nb_model, testing)
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracynb1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracynb1*100, 2), "%")
```

```{r}
nb_model <- naiveBayes(diagnosis ~ ., data = trainingSubset)
predictions <- predict(nb_model, testingSubset)
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracynb2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracynb2*100, 2), "%")
```

Logistic Regression

```{r}
lr_model <- glm(diagnosis ~ ., data = training, family = binomial)
predictions_prob <- predict(lr_model, testing, type = "response")
predictions <- ifelse(predictions_prob > 0.5, "M", "B")
predictions <- factor(predictions, levels = levels(testing$diagnosis))

confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracylr1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracylr1*100, 2), "%")
```

```{r}
lr_model <- glm(diagnosis ~ ., data = trainingSubset, family = binomial)
predictions_prob <- predict(lr_model, testingSubset, type = "response")
predictions <- ifelse(predictions_prob > 0.5, "M", "B")
predictions <- factor(predictions, levels = levels(testingSubset$diagnosis))

confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracylr2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracylr2*100, 2), "%")
```

Adaptive Boosting

```{r}
ab_model <- boosting(diagnosis ~ ., data = training, mfinal = 50)
predictions <- predict(ab_model, newdata = testing)

confusion_matrix <- table(Predicted = predictions$class, Actual = testing$diagnosis)
accuracyab1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracyab1*100, 2), "%")
```

```{r}
ab_model <- boosting(diagnosis ~ ., data = trainingSubset, mfinal = 50)
predictions <- predict(ab_model, newdata = testingSubset)

confusion_matrix <- table(Predicted = predictions$class, Actual = testingSubset$diagnosis)
accuracyab2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracyab2*100, 2), "%")
```

Gradient Boosting

```{r}
training1 <- training
testing1 <- testing
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

gb_model <- gbm(
  formula = diagnosis ~ ., 
  data = training1, 
  distribution = "bernoulli", 
  n.trees = 100, 
  interaction.depth = 3, 
  shrinkage = 0.01, 
  n.minobsinnode = 10
)


predictions_prob <- predict(gb_model, newdata = testing1, n.trees = 100, type = "response")
predictions <- ifelse(predictions_prob > 0.5, 1, 0) 

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracygb1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


print(confusion_matrix)
cat("Accuracy: ", round(accuracygb1 * 100, 2), "%\n")
```

```{r}
training1 <- trainingSubset
testing1 <- testingSubset
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

gb_model <- gbm(
  formula = diagnosis ~ ., 
  data = training1, 
  distribution = "bernoulli", 
  n.trees = 100, 
  interaction.depth = 3, 
  shrinkage = 0.01, 
  n.minobsinnode = 10
)

predictions_prob <- predict(gb_model, newdata = testing1, n.trees = 100, type = "response")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))


confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracygb2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

print(confusion_matrix)
cat("Accuracy: ", round(accuracygb2 * 100, 2), "%\n")
```

```{r}
training1 <- training
testing1 <- testing
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

mlp_model <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,       
  decay = 0.01,       
  maxit = 200,        
  linout = FALSE      
)

predictions_prob <- predict(mlp_model, newdata = testing1, type = "raw")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracymlp1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

print(confusion_matrix)
cat("Accuracy: ", round(accuracymlp1 * 100, 2), "%\n")
```

```{r}
training1 <- trainingSubset
testing1 <- testingSubset
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

mlp_model <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,       
  decay = 0.01,       
  maxit = 200,        
  linout = FALSE      
)

predictions_prob <- predict(mlp_model, newdata = testing1, type = "raw")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracymlp2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

print(confusion_matrix)
cat("Accuracy: ", round(accuracymlp2 * 100, 2), "%\n")
```

Neural Classification Chains

```{r}
training1 <- training
testing1 <- testing
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

model1 <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,          
  decay = 0.01,      
  maxit = 200,        
  linout = FALSE      
)

predictions_prob1 <- predict(model1, newdata = testing1, type = "raw")
testing1$pred1 <- ifelse(predictions_prob1 > 0.5, 1, 0)

train_predictions_prob1 <- predict(model1, newdata = training1, type = "raw")
training1$pred1 <- ifelse(train_predictions_prob1 > 0.5, 1, 0)

model2 <- nnet(
  diagnosis ~ . + pred1, 
  data = training1, 
  size = 10,        
  decay = 0.01,       
  maxit = 200,     
  linout = FALSE      
)

predictions_prob2 <- predict(model2, newdata = testing1, type = "raw")
predictions2 <- ifelse(predictions_prob2 > 0.5, 1, 0)

predictions2 <- factor(predictions2, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions2, Actual = actual)
accuracyncc1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

print(confusion_matrix)
cat("Accuracy: ", round(accuracyncc1 * 100, 2), "%\n")
```

```{r}
training1 <- trainingSubset
testing1 <- testingSubset
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

model1 <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,          
  decay = 0.01,      
  maxit = 200,        
  linout = FALSE      
)

predictions_prob1 <- predict(model1, newdata = testing1, type = "raw")
testing1$pred1 <- ifelse(predictions_prob1 > 0.5, 1, 0)

train_predictions_prob1 <- predict(model1, newdata = training1, type = "raw")
training1$pred1 <- ifelse(train_predictions_prob1 > 0.5, 1, 0)

model2 <- nnet(
  diagnosis ~ . + pred1, 
  data = training1, 
  size = 10,        
  decay = 0.01,       
  maxit = 200,     
  linout = FALSE      
)

predictions_prob2 <- predict(model2, newdata = testing1, type = "raw")
predictions2 <- ifelse(predictions_prob2 > 0.5, 1, 0)

predictions2 <- factor(predictions2, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions2, Actual = actual)
accuracyncc2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

print(confusion_matrix)
cat("Accuracy: ", round(accuracyncc2 * 100, 2), "%\n")
```

```{r}
cat("rf")
cat("Accuracy: ", round(accuracyrf1 * 100, 2), "%\n")
cat("Accuracy: ", round(accuracyrf2 * 100, 2), "%\n")
cat("knn")
cat("Accuracy: ", round(accuracyknn1 * 100, 2), "%\n")
cat("Accuracy: ", round(accuracyknn2 * 100, 2), "%\n")
cat("svm")
cat("Accuracy: ", round(accuracysvm1 * 100, 2), "%\n")
cat("Accuracy: ", round(accuracysvm2 * 100, 2), "%\n")
cat("dt")
cat("Accuracy: ", round(accuracydt1 * 100, 2), "%\n")
cat("Accuracy: ", round(accuracydt2 * 100, 2), "%\n")
cat("nb")
cat("Accuracy: ", round(accuracynb1 * 100, 2), "%\n")
cat("Accuracy: ", round(accuracynb2 * 100, 2), "%\n")
cat("lr")
cat("Accuracy: ", round(accuracylr1 * 100, 2), "%\n")
cat("Accuracy: ", round(accuracylr2 * 100, 2), "%\n")
cat("ab")
cat("Accuracy: ", round(accuracyab1 * 100, 2), "%\n")
cat("Accuracy: ", round(accuracyab2 * 100, 2), "%\n")
cat("gb")
cat("Accuracy: ", round(accuracygb1 * 100, 2), "%\n")
cat("Accuracy: ", round(accuracygb2 * 100, 2), "%\n")
cat("ncc")
cat("Accuracy: ", round(accuracyncc1 * 100, 2), "%\n")
cat("Accuracy: ", round(accuracyncc2 * 100, 2), "%\n")
cat("mlp")
cat("Accuracy: ", round(accuracymlp1 * 100, 2), "%\n")
cat("Accuracy: ", round(accuracymlp2 * 100, 2), "%\n")
```

```{r}
accuracies <- tibble(
  Model = c("rf", "knn", "svm", "dt", "nb", "lr", "ab", "gb", "ncc", "mlp"),
  All_Variables = c(round(accuracyrf1 * 100, 2), round(accuracyknn1 * 100, 2), round(accuracysvm1 * 100, 2),
                 round(accuracydt1 * 100, 2), round(accuracynb1 * 100, 2), round(accuracylr1 * 100, 2),
                 round(accuracyab1 * 100, 2), round(accuracygb1 * 100, 2), round(accuracyncc1 * 100, 2),
                 round(accuracymlp1 * 100, 2)),
  Subset = c(round(accuracyrf2 * 100, 2), round(accuracyknn2 * 100, 2), round(accuracysvm2 * 100, 2),
                 round(accuracydt2 * 100, 2), round(accuracynb2 * 100, 2), round(accuracylr2 * 100, 2),
                 round(accuracyab2 * 100, 2), round(accuracygb2 * 100, 2), round(accuracyncc2 * 100, 2),
                 round(accuracymlp2 * 100, 2))
)

print(accuracies)


```

```{r}
accuracies_long <- accuracies %>%
  pivot_longer(cols = c("All_Variables", "Subset"), 
               names_to = "Metric", 
               values_to = "Accuracy") %>%
  mutate(Metric = recode(Metric, "All_Variables" = "All", "Subset" = "Sub"))
print(accuracies_long)
```

```{r}

accuracies_dataframe_long <- accuracies %>%
  pivot_longer(cols = c("All_Variables", "Subset"), 
               names_to = "Metric", 
               values_to = "Accuracy") %>%
  mutate(Metric = recode(Metric, "All_Variables" = "All", "Subset" = "Sub"))
print(accuracies_dataframe_long)

ggplot(accuracies_dataframe_long, aes(x = Metric, y = Accuracy)) +
  geom_boxplot(outlier.shape = NA) +  # Boxplot without outliers
  geom_jitter(width = 0.2, size = 2,aes(color = Model)) +  # Add jitter points
  labs(title = "Model Accuracy Comparison",
       x = "Metric",
       y = "Accuracy (%)") +
  theme_minimal()
```

```{r}
big_accuracy_dataframe <- tibble()

for(seed in 1:200){
  set.seed(seed)
dataset <- data
split <- sample.split(dataset$diagnosis,SplitRatio=0.7)
training <- subset(dataset,split==TRUE)
testing <- subset(dataset,split==FALSE)
training$diagnosis <- as.factor(training$diagnosis)
testing$diagnosis <- as.factor(testing$diagnosis)



trainingSubset <- training %>% select(-c("concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean"))
testingSubset <- testing %>% select(-c("concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean"))



rf_model <- randomForest(x = training[,-1],
                         y = training$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testing[,-1])

actuals <- testing$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracyrf1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


rf_model <- randomForest(x = trainingSubset[,-1],
                         y = trainingSubset$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testingSubset[,-1])

actuals <- testingSubset$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracyrf2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


knn_model <- knn(train = training[,-1], test=testing[,-1],cl=training$diagnosis, k=5)

actuals <- testing$diagnosis

confusion_matrix <- table(Predicted = knn_model, Actual = actuals)

accuracyknn1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


knn_model <- knn(train = trainingSubset[,-1], test=testingSubset[,-1],cl=training$diagnosis, k=5)

actuals <- testingSubset$diagnosis

confusion_matrix <- table(Predicted = knn_model, Actual = actuals)

accuracyknn2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



svm_model <- svm(diagnosis ~ ., data = training, kernel = "radial", cost = 1, scale = TRUE)

predictions <- predict(svm_model, testing)
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)

accuracysvm1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


svm_model <- svm(diagnosis ~ ., data = trainingSubset, kernel = "radial", cost = 1, scale = TRUE)

predictions <- predict(svm_model, testingSubset)
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)

accuracysvm2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


dt_model <- rpart(diagnosis ~ ., data = training, method = "class")
predictions <- predict(dt_model, testing, type = "class")
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracydt1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



dt_model <- rpart(diagnosis ~ ., data = trainingSubset, method = "class")
predictions <- predict(dt_model, testingSubset, type = "class")
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracydt2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



nb_model <- naiveBayes(diagnosis ~ ., data = training)
predictions <- predict(nb_model, testing)
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracynb1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



nb_model <- naiveBayes(diagnosis ~ ., data = trainingSubset)
predictions <- predict(nb_model, testingSubset)
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracynb2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



lr_model <- glm(diagnosis ~ ., data = training, family = binomial)
predictions_prob <- predict(lr_model, testing, type = "response")
predictions <- ifelse(predictions_prob > 0.5, "M", "B")
predictions <- factor(predictions, levels = levels(testing$diagnosis))

confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracylr1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



lr_model <- glm(diagnosis ~ ., data = trainingSubset, family = binomial)
predictions_prob <- predict(lr_model, testingSubset, type = "response")
predictions <- ifelse(predictions_prob > 0.5, "M", "B")
predictions <- factor(predictions, levels = levels(testingSubset$diagnosis))

confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracylr2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



ab_model <- boosting(diagnosis ~ ., data = training, mfinal = 50)
predictions <- predict(ab_model, newdata = testing)

confusion_matrix <- table(Predicted = predictions$class, Actual = testing$diagnosis)
accuracyab1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



ab_model <- boosting(diagnosis ~ ., data = trainingSubset, mfinal = 50)
predictions <- predict(ab_model, newdata = testingSubset)

confusion_matrix <- table(Predicted = predictions$class, Actual = testingSubset$diagnosis)
accuracyab2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



training1 <- training
testing1 <- testing
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

gb_model <- gbm(
  formula = diagnosis ~ ., 
  data = training1, 
  distribution = "bernoulli", 
  n.trees = 100, 
  interaction.depth = 3, 
  shrinkage = 0.01, 
  n.minobsinnode = 10
)


predictions_prob <- predict(gb_model, newdata = testing1, n.trees = 100, type = "response")
predictions <- ifelse(predictions_prob > 0.5, 1, 0) 

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracygb1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



gb_model <- gbm(
  formula = diagnosis ~ ., 
  data = training1, 
  distribution = "bernoulli", 
  n.trees = 100, 
  interaction.depth = 3, 
  shrinkage = 0.01, 
  n.minobsinnode = 10
)

predictions_prob <- predict(gb_model, newdata = testing1, n.trees = 100, type = "response")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))


confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracygb2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



mlp_model <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,       
  decay = 0.01,       
  maxit = 200,        
  linout = FALSE      
)

predictions_prob <- predict(mlp_model, newdata = testing1, type = "raw")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracymlp1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



mlp_model <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,       
  decay = 0.01,       
  maxit = 200,        
  linout = FALSE      
)

predictions_prob <- predict(mlp_model, newdata = testing1, type = "raw")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracymlp2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



model1 <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,          
  decay = 0.01,      
  maxit = 200,        
  linout = FALSE      
)

predictions_prob1 <- predict(model1, newdata = testing1, type = "raw")
testing1$pred1 <- ifelse(predictions_prob1 > 0.5, 1, 0)

train_predictions_prob1 <- predict(model1, newdata = training1, type = "raw")
training1$pred1 <- ifelse(train_predictions_prob1 > 0.5, 1, 0)

model2 <- nnet(
  diagnosis ~ . + pred1, 
  data = training1, 
  size = 10,        
  decay = 0.01,       
  maxit = 200,     
  linout = FALSE      
)

predictions_prob2 <- predict(model2, newdata = testing1, type = "raw")
predictions2 <- ifelse(predictions_prob2 > 0.5, 1, 0)

predictions2 <- factor(predictions2, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions2, Actual = actual)
accuracyncc1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



model1 <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,          
  decay = 0.01,      
  maxit = 200,        
  linout = FALSE      
)

predictions_prob1 <- predict(model1, newdata = testing1, type = "raw")
testing1$pred1 <- ifelse(predictions_prob1 > 0.5, 1, 0)

train_predictions_prob1 <- predict(model1, newdata = training1, type = "raw")
training1$pred1 <- ifelse(train_predictions_prob1 > 0.5, 1, 0)

model2 <- nnet(
  diagnosis ~ . + pred1, 
  data = training1, 
  size = 10,        
  decay = 0.01,       
  maxit = 200,     
  linout = FALSE      
)

predictions_prob2 <- predict(model2, newdata = testing1, type = "raw")
predictions2 <- ifelse(predictions_prob2 > 0.5, 1, 0)

predictions2 <- factor(predictions2, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions2, Actual = actual)
accuracyncc2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


accuracies <- tibble(
  Model = c("rf", "knn", "svm", "dt", "nb", "lr", "ab", "gb", "ncc", "mlp"),
  All_Variables = c(round(accuracyrf1 * 100, 2), round(accuracyknn1 * 100, 2), round(accuracysvm1 * 100, 2),
                    round(accuracydt1 * 100, 2), round(accuracynb1 * 100, 2), round(accuracylr1 * 100, 2),
                    round(accuracyab1 * 100, 2), round(accuracygb1 * 100, 2), round(accuracyncc1 * 100, 2),
                    round(accuracymlp1 * 100, 2)),
  Subset = c(round(accuracyrf2 * 100, 2), round(accuracyknn2 * 100, 2), round(accuracysvm2 * 100, 2),
             round(accuracydt2 * 100, 2), round(accuracynb2 * 100, 2), round(accuracylr2 * 100, 2),
             round(accuracyab2 * 100, 2), round(accuracygb2 * 100, 2), round(accuracyncc2 * 100, 2),
             round(accuracymlp2 * 100, 2))
)

accuracies_long <- accuracies %>%
  pivot_longer(cols = c("All_Variables", "Subset"), 
               names_to = "Metric", 
               values_to = "Accuracy") %>%
  mutate(Metric = recode(Metric, "All_Variables" = "All", "Subset" = "Sub"))
accuracies_long$seed = seed
big_accuracy_dataframe <- bind_rows(big_accuracy_dataframe,accuracies_long)

}

#big_accuracy_dataframe <- big_accuracy_dataframe %>% mutate(simulation = row_number())

```

```{r}
print(big_accuracy_dataframe)


ggplot(big_accuracy_dataframe, aes(x = Metric, y = Accuracy)) +
  geom_boxplot(outlier.shape = NA) +  # Boxplot without outliers
  geom_jitter(width = 0.2, size = 2,alpha=0.3,aes(color = Model)) +  # Add jitter points
  labs(title = "Model Accuracy Comparison",
       x = "Metric",
       y = "Accuracy (%)") +
  theme_minimal()
```

We see that the subset is very similar and barely better than using all the elements. This means that we don't really need the ones that we got rid of earlier to get very similar results.

```{r}
big_accuracy_dataframe <- big_accuracy_dataframe
for(i in 1:199){
  part_of_data <- big_accuracy_dataframe %>% filter(seed == i)
  part_of_data$seed = i+1
  big_accuracy_dataframe <- rbind(big_accuracy_dataframe,part_of_data)
}
print(big_accuracy_dataframe)
```

```{r}
p <- ggplot(big_accuracy_dataframe, aes(x = Metric, y = Accuracy)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(width = 0.2, size = 2, alpha = 0.3, aes(color = Model, group = interaction(Model, Metric))) +
  labs(title = "Model Accuracy Comparison",
       x = "Metric",
       y = "Cumulative Accuracy (%)") +
  theme_minimal() +
  transition_states(seed, transition_length = 2, state_length = 1, wrap = FALSE) +
  enter_fade() +
  exit_fade() +
  labs(subtitle = "Seed: {closest_state}")

# Animate
anim <- animate(p, duration = 10, fps = 20, width = 800, height = 600, renderer = gifski_renderer())

# Save the animation
anim_save("model_accuracy_comparison_cumulative.gif", anim)
```

```{r}
anim
```

```{r}
names(trainingSubset)
```

```{r}
big_accuracy_dataframe2 <- tibble()

for(seed in 1:200){
  set.seed(seed)
dataset <- data
split <- sample.split(dataset$diagnosis,SplitRatio=0.7)
training <- subset(dataset,split==TRUE)
testing <- subset(dataset,split==FALSE)
training$diagnosis <- as.factor(training$diagnosis)
testing$diagnosis <- as.factor(testing$diagnosis)

trainingSubset <- training %>% select(-c("concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean"))
testingSubset <- testing %>% select(-c("concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean"))

trainingSubset <- trainingSubset %>% select(c("diagnosis","concave.points_worst","radius_worst","concavity_worst","radius_se","compactness_mean"))
testingSubset <- testingSubset %>% select(c("diagnosis","concave.points_worst","radius_worst","concavity_worst","radius_se","compactness_mean"))

training <- training %>% select(c("diagnosis","radius_worst","concave.points_worst","perimeter_worst","concave.points_mean","area_worst","radius_mean"))
testing <- testing %>% select(c("diagnosis","radius_worst","concave.points_worst","perimeter_worst","concave.points_mean","area_worst","radius_mean"))



rf_model <- randomForest(x = training[,-1],
                         y = training$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testing[,-1])

actuals <- testing$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracyrf1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


rf_model <- randomForest(x = trainingSubset[,-1],
                         y = trainingSubset$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testingSubset[,-1])

actuals <- testingSubset$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracyrf2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


knn_model <- knn(train = training[,-1], test=testing[,-1],cl=training$diagnosis, k=5)

actuals <- testing$diagnosis

confusion_matrix <- table(Predicted = knn_model, Actual = actuals)

accuracyknn1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


knn_model <- knn(train = trainingSubset[,-1], test=testingSubset[,-1],cl=training$diagnosis, k=5)

actuals <- testingSubset$diagnosis

confusion_matrix <- table(Predicted = knn_model, Actual = actuals)

accuracyknn2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



svm_model <- svm(diagnosis ~ ., data = training, kernel = "radial", cost = 1, scale = TRUE)

predictions <- predict(svm_model, testing)
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)

accuracysvm1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


svm_model <- svm(diagnosis ~ ., data = trainingSubset, kernel = "radial", cost = 1, scale = TRUE)

predictions <- predict(svm_model, testingSubset)
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)

accuracysvm2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


dt_model <- rpart(diagnosis ~ ., data = training, method = "class")
predictions <- predict(dt_model, testing, type = "class")
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracydt1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



dt_model <- rpart(diagnosis ~ ., data = trainingSubset, method = "class")
predictions <- predict(dt_model, testingSubset, type = "class")
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracydt2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



nb_model <- naiveBayes(diagnosis ~ ., data = training)
predictions <- predict(nb_model, testing)
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracynb1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



nb_model <- naiveBayes(diagnosis ~ ., data = trainingSubset)
predictions <- predict(nb_model, testingSubset)
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracynb2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



lr_model <- glm(diagnosis ~ ., data = training, family = binomial)
predictions_prob <- predict(lr_model, testing, type = "response")
predictions <- ifelse(predictions_prob > 0.5, "M", "B")
predictions <- factor(predictions, levels = levels(testing$diagnosis))

confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracylr1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



lr_model <- glm(diagnosis ~ ., data = trainingSubset, family = binomial)
predictions_prob <- predict(lr_model, testingSubset, type = "response")
predictions <- ifelse(predictions_prob > 0.5, "M", "B")
predictions <- factor(predictions, levels = levels(testingSubset$diagnosis))

confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracylr2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



ab_model <- boosting(diagnosis ~ ., data = training, mfinal = 50)
predictions <- predict(ab_model, newdata = testing)

confusion_matrix <- table(Predicted = predictions$class, Actual = testing$diagnosis)
accuracyab1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



ab_model <- boosting(diagnosis ~ ., data = trainingSubset, mfinal = 50)
predictions <- predict(ab_model, newdata = testingSubset)

confusion_matrix <- table(Predicted = predictions$class, Actual = testingSubset$diagnosis)
accuracyab2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



training1 <- training
testing1 <- testing
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

gb_model <- gbm(
  formula = diagnosis ~ ., 
  data = training1, 
  distribution = "bernoulli", 
  n.trees = 100, 
  interaction.depth = 3, 
  shrinkage = 0.01, 
  n.minobsinnode = 10
)


predictions_prob <- predict(gb_model, newdata = testing1, n.trees = 100, type = "response")
predictions <- ifelse(predictions_prob > 0.5, 1, 0) 

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracygb1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



gb_model <- gbm(
  formula = diagnosis ~ ., 
  data = training1, 
  distribution = "bernoulli", 
  n.trees = 100, 
  interaction.depth = 3, 
  shrinkage = 0.01, 
  n.minobsinnode = 10
)

predictions_prob <- predict(gb_model, newdata = testing1, n.trees = 100, type = "response")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))


confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracygb2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



mlp_model <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,       
  decay = 0.01,       
  maxit = 200,        
  linout = FALSE      
)

predictions_prob <- predict(mlp_model, newdata = testing1, type = "raw")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracymlp1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



mlp_model <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,       
  decay = 0.01,       
  maxit = 200,        
  linout = FALSE      
)

predictions_prob <- predict(mlp_model, newdata = testing1, type = "raw")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracymlp2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



model1 <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,          
  decay = 0.01,      
  maxit = 200,        
  linout = FALSE      
)

predictions_prob1 <- predict(model1, newdata = testing1, type = "raw")
testing1$pred1 <- ifelse(predictions_prob1 > 0.5, 1, 0)

train_predictions_prob1 <- predict(model1, newdata = training1, type = "raw")
training1$pred1 <- ifelse(train_predictions_prob1 > 0.5, 1, 0)

model2 <- nnet(
  diagnosis ~ . + pred1, 
  data = training1, 
  size = 10,        
  decay = 0.01,       
  maxit = 200,     
  linout = FALSE      
)

predictions_prob2 <- predict(model2, newdata = testing1, type = "raw")
predictions2 <- ifelse(predictions_prob2 > 0.5, 1, 0)

predictions2 <- factor(predictions2, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions2, Actual = actual)
accuracyncc1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



model1 <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,          
  decay = 0.01,      
  maxit = 200,        
  linout = FALSE      
)

predictions_prob1 <- predict(model1, newdata = testing1, type = "raw")
testing1$pred1 <- ifelse(predictions_prob1 > 0.5, 1, 0)

train_predictions_prob1 <- predict(model1, newdata = training1, type = "raw")
training1$pred1 <- ifelse(train_predictions_prob1 > 0.5, 1, 0)

model2 <- nnet(
  diagnosis ~ . + pred1, 
  data = training1, 
  size = 10,        
  decay = 0.01,       
  maxit = 200,     
  linout = FALSE      
)

predictions_prob2 <- predict(model2, newdata = testing1, type = "raw")
predictions2 <- ifelse(predictions_prob2 > 0.5, 1, 0)

predictions2 <- factor(predictions2, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions2, Actual = actual)
accuracyncc2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


accuracies <- tibble(
  Model = c("rf", "knn", "svm", "dt", "nb", "lr", "ab", "gb", "ncc", "mlp"),
  All_Variables = c(round(accuracyrf1 * 100, 2), round(accuracyknn1 * 100, 2), round(accuracysvm1 * 100, 2),
                    round(accuracydt1 * 100, 2), round(accuracynb1 * 100, 2), round(accuracylr1 * 100, 2),
                    round(accuracyab1 * 100, 2), round(accuracygb1 * 100, 2), round(accuracyncc1 * 100, 2),
                    round(accuracymlp1 * 100, 2)),
  Subset = c(round(accuracyrf2 * 100, 2), round(accuracyknn2 * 100, 2), round(accuracysvm2 * 100, 2),
             round(accuracydt2 * 100, 2), round(accuracynb2 * 100, 2), round(accuracylr2 * 100, 2),
             round(accuracyab2 * 100, 2), round(accuracygb2 * 100, 2), round(accuracyncc2 * 100, 2),
             round(accuracymlp2 * 100, 2))
)

accuracies_long <- accuracies %>%
  pivot_longer(cols = c("All_Variables", "Subset"), 
               names_to = "Metric", 
               values_to = "Accuracy") %>%
  mutate(Metric = recode(Metric, "All_Variables" = "Fea", "Subset" = "SubFea"))
accuracies_long$seed = seed
big_accuracy_dataframe2 <- bind_rows(big_accuracy_dataframe2,accuracies_long)

}

#big_accuracy_dataframe <- big_accuracy_dataframe %>% mutate(simulation = row_number())

```

```{r}
big_accuracy_dataframe2 <- big_accuracy_dataframe2
for(i in 1:199){
  part_of_data <- big_accuracy_dataframe2 %>% filter(seed == i)
  part_of_data$seed = i+1
  big_accuracy_dataframe2 <- rbind(big_accuracy_dataframe2,part_of_data)
}
print(big_accuracy_dataframe2)
```

```{r}
p <- ggplot(big_accuracy_dataframe2, aes(x = Metric, y = Accuracy)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(width = 0.2, size = 2, alpha = 0.3, aes(color = Model, group = interaction(Model, Metric))) +
  labs(title = "Model Accuracy Comparison",
       x = "Metric",
       y = "Cumulative Accuracy (%)") +
  theme_minimal() +
  transition_states(seed, transition_length = 2, state_length = 1, wrap = FALSE) +
  enter_fade() +
  exit_fade() +
  labs(subtitle = "Seed: {closest_state}")

# Animate
anim <- animate(p, duration = 10, fps = 20, width = 800, height = 600, renderer = gifski_renderer())

# Save the animation
anim_save("model_accuracy_comparison_cumulative.gif", anim)
```

Combine features with the defaults

```{r}
final_accuracies <- rbind(big_accuracy_dataframe,big_accuracy_dataframe2)
```

```{r}

p <- ggplot(final_accuracies, aes(x = Metric, y = Accuracy)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(width = 0.2, size = 2, alpha = 0.3, aes(color = Model, group = interaction(Model, Metric))) +
  labs(title = "Model Accuracy Comparison",
       x = "Metric",
       y = "Cumulative Accuracy (%)") +
  theme_minimal() +
  transition_states(seed, transition_length = 2, state_length = 1, wrap = FALSE) +
  enter_fade() +
  exit_fade() +
  labs(subtitle = "Seed: {closest_state}")

# Animate
anim <- animate(p, duration = 10, fps = 20, width = 800, height = 600, renderer = gifski_renderer())

# Save the animation
anim_save("model_accuracy_comparison_cumulative.gif", anim)
```

```{r}
anim
```

What if we standardize the variables and run this all once again?

```{r}
print(data)
```

```{r}
data_standard <- cbind(data[1],as.data.frame(scale(data[2:31])))
```

```{r}
big_accuracy_dataframe_standard <- tibble()

for(seed in 1:200){
  set.seed(seed)
dataset <- data_standard
split <- sample.split(dataset$diagnosis,SplitRatio=0.7)
training <- subset(dataset,split==TRUE)
testing <- subset(dataset,split==FALSE)
training$diagnosis <- as.factor(training$diagnosis)
testing$diagnosis <- as.factor(testing$diagnosis)



trainingSubset <- training %>% select(-c("concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean"))
testingSubset <- testing %>% select(-c("concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean"))



rf_model <- randomForest(x = training[,-1],
                         y = training$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testing[,-1])

actuals <- testing$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracyrf1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


rf_model <- randomForest(x = trainingSubset[,-1],
                         y = trainingSubset$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testingSubset[,-1])

actuals <- testingSubset$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracyrf2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


knn_model <- knn(train = training[,-1], test=testing[,-1],cl=training$diagnosis, k=5)

actuals <- testing$diagnosis

confusion_matrix <- table(Predicted = knn_model, Actual = actuals)

accuracyknn1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


knn_model <- knn(train = trainingSubset[,-1], test=testingSubset[,-1],cl=training$diagnosis, k=5)

actuals <- testingSubset$diagnosis

confusion_matrix <- table(Predicted = knn_model, Actual = actuals)

accuracyknn2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



svm_model <- svm(diagnosis ~ ., data = training, kernel = "radial", cost = 1, scale = TRUE)

predictions <- predict(svm_model, testing)
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)

accuracysvm1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


svm_model <- svm(diagnosis ~ ., data = trainingSubset, kernel = "radial", cost = 1, scale = TRUE)

predictions <- predict(svm_model, testingSubset)
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)

accuracysvm2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


dt_model <- rpart(diagnosis ~ ., data = training, method = "class")
predictions <- predict(dt_model, testing, type = "class")
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracydt1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



dt_model <- rpart(diagnosis ~ ., data = trainingSubset, method = "class")
predictions <- predict(dt_model, testingSubset, type = "class")
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracydt2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



nb_model <- naiveBayes(diagnosis ~ ., data = training)
predictions <- predict(nb_model, testing)
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracynb1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



nb_model <- naiveBayes(diagnosis ~ ., data = trainingSubset)
predictions <- predict(nb_model, testingSubset)
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracynb2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



lr_model <- glm(diagnosis ~ ., data = training, family = binomial)
predictions_prob <- predict(lr_model, testing, type = "response")
predictions <- ifelse(predictions_prob > 0.5, "M", "B")
predictions <- factor(predictions, levels = levels(testing$diagnosis))

confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracylr1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



lr_model <- glm(diagnosis ~ ., data = trainingSubset, family = binomial)
predictions_prob <- predict(lr_model, testingSubset, type = "response")
predictions <- ifelse(predictions_prob > 0.5, "M", "B")
predictions <- factor(predictions, levels = levels(testingSubset$diagnosis))

confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracylr2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



ab_model <- boosting(diagnosis ~ ., data = training, mfinal = 50)
predictions <- predict(ab_model, newdata = testing)

confusion_matrix <- table(Predicted = predictions$class, Actual = testing$diagnosis)
accuracyab1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



ab_model <- boosting(diagnosis ~ ., data = trainingSubset, mfinal = 50)
predictions <- predict(ab_model, newdata = testingSubset)

confusion_matrix <- table(Predicted = predictions$class, Actual = testingSubset$diagnosis)
accuracyab2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



training1 <- training
testing1 <- testing
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

gb_model <- gbm(
  formula = diagnosis ~ ., 
  data = training1, 
  distribution = "bernoulli", 
  n.trees = 100, 
  interaction.depth = 3, 
  shrinkage = 0.01, 
  n.minobsinnode = 10
)


predictions_prob <- predict(gb_model, newdata = testing1, n.trees = 100, type = "response")
predictions <- ifelse(predictions_prob > 0.5, 1, 0) 

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracygb1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



gb_model <- gbm(
  formula = diagnosis ~ ., 
  data = training1, 
  distribution = "bernoulli", 
  n.trees = 100, 
  interaction.depth = 3, 
  shrinkage = 0.01, 
  n.minobsinnode = 10
)

predictions_prob <- predict(gb_model, newdata = testing1, n.trees = 100, type = "response")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))


confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracygb2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



mlp_model <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,       
  decay = 0.01,       
  maxit = 200,        
  linout = FALSE      
)

predictions_prob <- predict(mlp_model, newdata = testing1, type = "raw")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracymlp1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



mlp_model <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,       
  decay = 0.01,       
  maxit = 200,        
  linout = FALSE      
)

predictions_prob <- predict(mlp_model, newdata = testing1, type = "raw")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracymlp2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



model1 <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,          
  decay = 0.01,      
  maxit = 200,        
  linout = FALSE      
)

predictions_prob1 <- predict(model1, newdata = testing1, type = "raw")
testing1$pred1 <- ifelse(predictions_prob1 > 0.5, 1, 0)

train_predictions_prob1 <- predict(model1, newdata = training1, type = "raw")
training1$pred1 <- ifelse(train_predictions_prob1 > 0.5, 1, 0)

model2 <- nnet(
  diagnosis ~ . + pred1, 
  data = training1, 
  size = 10,        
  decay = 0.01,       
  maxit = 200,     
  linout = FALSE      
)

predictions_prob2 <- predict(model2, newdata = testing1, type = "raw")
predictions2 <- ifelse(predictions_prob2 > 0.5, 1, 0)

predictions2 <- factor(predictions2, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions2, Actual = actual)
accuracyncc1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



model1 <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,          
  decay = 0.01,      
  maxit = 200,        
  linout = FALSE      
)

predictions_prob1 <- predict(model1, newdata = testing1, type = "raw")
testing1$pred1 <- ifelse(predictions_prob1 > 0.5, 1, 0)

train_predictions_prob1 <- predict(model1, newdata = training1, type = "raw")
training1$pred1 <- ifelse(train_predictions_prob1 > 0.5, 1, 0)

model2 <- nnet(
  diagnosis ~ . + pred1, 
  data = training1, 
  size = 10,        
  decay = 0.01,       
  maxit = 200,     
  linout = FALSE      
)

predictions_prob2 <- predict(model2, newdata = testing1, type = "raw")
predictions2 <- ifelse(predictions_prob2 > 0.5, 1, 0)

predictions2 <- factor(predictions2, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions2, Actual = actual)
accuracyncc2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


accuracies <- tibble(
  Model = c("rf", "knn", "svm", "dt", "nb", "lr", "ab", "gb", "ncc", "mlp"),
  All_Variables = c(round(accuracyrf1 * 100, 2), round(accuracyknn1 * 100, 2), round(accuracysvm1 * 100, 2),
                    round(accuracydt1 * 100, 2), round(accuracynb1 * 100, 2), round(accuracylr1 * 100, 2),
                    round(accuracyab1 * 100, 2), round(accuracygb1 * 100, 2), round(accuracyncc1 * 100, 2),
                    round(accuracymlp1 * 100, 2)),
  Subset = c(round(accuracyrf2 * 100, 2), round(accuracyknn2 * 100, 2), round(accuracysvm2 * 100, 2),
             round(accuracydt2 * 100, 2), round(accuracynb2 * 100, 2), round(accuracylr2 * 100, 2),
             round(accuracyab2 * 100, 2), round(accuracygb2 * 100, 2), round(accuracyncc2 * 100, 2),
             round(accuracymlp2 * 100, 2))
)

accuracies_long <- accuracies %>%
  pivot_longer(cols = c("All_Variables", "Subset"), 
               names_to = "Metric", 
               values_to = "Accuracy") %>%
  mutate(Metric = recode(Metric, "All_Variables" = "All", "Subset" = "Sub"))
accuracies_long$seed = seed
big_accuracy_dataframe_standard <- bind_rows(big_accuracy_dataframe_standard,accuracies_long)

}
```

```{r}
big_accuracy_dataframe2_standard <- tibble()

for(seed in 1:200){
  set.seed(seed)
dataset <- data_standard
split <- sample.split(dataset$diagnosis,SplitRatio=0.7)
training <- subset(dataset,split==TRUE)
testing <- subset(dataset,split==FALSE)
training$diagnosis <- as.factor(training$diagnosis)
testing$diagnosis <- as.factor(testing$diagnosis)

trainingSubset <- training %>% select(-c("concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean"))
testingSubset <- testing %>% select(-c("concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean"))

trainingSubset <- trainingSubset %>% select(c("diagnosis","concave.points_worst","radius_worst","concavity_worst","radius_se","compactness_mean"))
testingSubset <- testingSubset %>% select(c("diagnosis","concave.points_worst","radius_worst","concavity_worst","radius_se","compactness_mean"))

training <- training %>% select(c("diagnosis","radius_worst","concave.points_worst","perimeter_worst","concave.points_mean","area_worst","radius_mean"))
testing <- testing %>% select(c("diagnosis","radius_worst","concave.points_worst","perimeter_worst","concave.points_mean","area_worst","radius_mean"))



rf_model <- randomForest(x = training[,-1],
                         y = training$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testing[,-1])

actuals <- testing$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracyrf1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


rf_model <- randomForest(x = trainingSubset[,-1],
                         y = trainingSubset$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testingSubset[,-1])

actuals <- testingSubset$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracyrf2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


knn_model <- knn(train = training[,-1], test=testing[,-1],cl=training$diagnosis, k=5)

actuals <- testing$diagnosis

confusion_matrix <- table(Predicted = knn_model, Actual = actuals)

accuracyknn1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


knn_model <- knn(train = trainingSubset[,-1], test=testingSubset[,-1],cl=training$diagnosis, k=5)

actuals <- testingSubset$diagnosis

confusion_matrix <- table(Predicted = knn_model, Actual = actuals)

accuracyknn2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



svm_model <- svm(diagnosis ~ ., data = training, kernel = "radial", cost = 1, scale = TRUE)

predictions <- predict(svm_model, testing)
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)

accuracysvm1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


svm_model <- svm(diagnosis ~ ., data = trainingSubset, kernel = "radial", cost = 1, scale = TRUE)

predictions <- predict(svm_model, testingSubset)
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)

accuracysvm2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


dt_model <- rpart(diagnosis ~ ., data = training, method = "class")
predictions <- predict(dt_model, testing, type = "class")
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracydt1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



dt_model <- rpart(diagnosis ~ ., data = trainingSubset, method = "class")
predictions <- predict(dt_model, testingSubset, type = "class")
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracydt2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



nb_model <- naiveBayes(diagnosis ~ ., data = training)
predictions <- predict(nb_model, testing)
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracynb1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



nb_model <- naiveBayes(diagnosis ~ ., data = trainingSubset)
predictions <- predict(nb_model, testingSubset)
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracynb2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



lr_model <- glm(diagnosis ~ ., data = training, family = binomial)
predictions_prob <- predict(lr_model, testing, type = "response")
predictions <- ifelse(predictions_prob > 0.5, "M", "B")
predictions <- factor(predictions, levels = levels(testing$diagnosis))

confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracylr1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



lr_model <- glm(diagnosis ~ ., data = trainingSubset, family = binomial)
predictions_prob <- predict(lr_model, testingSubset, type = "response")
predictions <- ifelse(predictions_prob > 0.5, "M", "B")
predictions <- factor(predictions, levels = levels(testingSubset$diagnosis))

confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracylr2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



ab_model <- boosting(diagnosis ~ ., data = training, mfinal = 50)
predictions <- predict(ab_model, newdata = testing)

confusion_matrix <- table(Predicted = predictions$class, Actual = testing$diagnosis)
accuracyab1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



ab_model <- boosting(diagnosis ~ ., data = trainingSubset, mfinal = 50)
predictions <- predict(ab_model, newdata = testingSubset)

confusion_matrix <- table(Predicted = predictions$class, Actual = testingSubset$diagnosis)
accuracyab2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



training1 <- training
testing1 <- testing
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

gb_model <- gbm(
  formula = diagnosis ~ ., 
  data = training1, 
  distribution = "bernoulli", 
  n.trees = 100, 
  interaction.depth = 3, 
  shrinkage = 0.01, 
  n.minobsinnode = 10
)


predictions_prob <- predict(gb_model, newdata = testing1, n.trees = 100, type = "response")
predictions <- ifelse(predictions_prob > 0.5, 1, 0) 

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracygb1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



gb_model <- gbm(
  formula = diagnosis ~ ., 
  data = training1, 
  distribution = "bernoulli", 
  n.trees = 100, 
  interaction.depth = 3, 
  shrinkage = 0.01, 
  n.minobsinnode = 10
)

predictions_prob <- predict(gb_model, newdata = testing1, n.trees = 100, type = "response")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))


confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracygb2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



mlp_model <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,       
  decay = 0.01,       
  maxit = 200,        
  linout = FALSE      
)

predictions_prob <- predict(mlp_model, newdata = testing1, type = "raw")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracymlp1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



mlp_model <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,       
  decay = 0.01,       
  maxit = 200,        
  linout = FALSE      
)

predictions_prob <- predict(mlp_model, newdata = testing1, type = "raw")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracymlp2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



model1 <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,          
  decay = 0.01,      
  maxit = 200,        
  linout = FALSE      
)

predictions_prob1 <- predict(model1, newdata = testing1, type = "raw")
testing1$pred1 <- ifelse(predictions_prob1 > 0.5, 1, 0)

train_predictions_prob1 <- predict(model1, newdata = training1, type = "raw")
training1$pred1 <- ifelse(train_predictions_prob1 > 0.5, 1, 0)

model2 <- nnet(
  diagnosis ~ . + pred1, 
  data = training1, 
  size = 10,        
  decay = 0.01,       
  maxit = 200,     
  linout = FALSE      
)

predictions_prob2 <- predict(model2, newdata = testing1, type = "raw")
predictions2 <- ifelse(predictions_prob2 > 0.5, 1, 0)

predictions2 <- factor(predictions2, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions2, Actual = actual)
accuracyncc1 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)



model1 <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,          
  decay = 0.01,      
  maxit = 200,        
  linout = FALSE      
)

predictions_prob1 <- predict(model1, newdata = testing1, type = "raw")
testing1$pred1 <- ifelse(predictions_prob1 > 0.5, 1, 0)

train_predictions_prob1 <- predict(model1, newdata = training1, type = "raw")
training1$pred1 <- ifelse(train_predictions_prob1 > 0.5, 1, 0)

model2 <- nnet(
  diagnosis ~ . + pred1, 
  data = training1, 
  size = 10,        
  decay = 0.01,       
  maxit = 200,     
  linout = FALSE      
)

predictions_prob2 <- predict(model2, newdata = testing1, type = "raw")
predictions2 <- ifelse(predictions_prob2 > 0.5, 1, 0)

predictions2 <- factor(predictions2, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions2, Actual = actual)
accuracyncc2 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


accuracies <- tibble(
  Model = c("rf", "knn", "svm", "dt", "nb", "lr", "ab", "gb", "ncc", "mlp"),
  All_Variables = c(round(accuracyrf1 * 100, 2), round(accuracyknn1 * 100, 2), round(accuracysvm1 * 100, 2),
                    round(accuracydt1 * 100, 2), round(accuracynb1 * 100, 2), round(accuracylr1 * 100, 2),
                    round(accuracyab1 * 100, 2), round(accuracygb1 * 100, 2), round(accuracyncc1 * 100, 2),
                    round(accuracymlp1 * 100, 2)),
  Subset = c(round(accuracyrf2 * 100, 2), round(accuracyknn2 * 100, 2), round(accuracysvm2 * 100, 2),
             round(accuracydt2 * 100, 2), round(accuracynb2 * 100, 2), round(accuracylr2 * 100, 2),
             round(accuracyab2 * 100, 2), round(accuracygb2 * 100, 2), round(accuracyncc2 * 100, 2),
             round(accuracymlp2 * 100, 2))
)

accuracies_long <- accuracies %>%
  pivot_longer(cols = c("All_Variables", "Subset"), 
               names_to = "Metric", 
               values_to = "Accuracy") %>%
  mutate(Metric = recode(Metric, "All_Variables" = "Fea", "Subset" = "SubFea"))
accuracies_long$seed = seed
big_accuracy_dataframe2_standard <- bind_rows(big_accuracy_dataframe2_standard,accuracies_long)

}

#big_accuracy_dataframe <- big_accuracy_dataframe %>% mutate(simulation = row_number())

```

```{r}
print(big_accuracy_dataframe_standard)
big_accuracy_dataframe_standard <- big_accuracy_dataframe_standard
for(i in 1:199){
  part_of_data <- big_accuracy_dataframe_standard %>% filter(seed == i)
  part_of_data$seed = i+1
  big_accuracy_dataframe_standard <- rbind(big_accuracy_dataframe_standard,part_of_data)
}
big_accuracy_dataframe_standard
```

```{r}
big_accuracy_dataframe2_standard <- big_accuracy_dataframe2_standard
for(i in 1:199){
  part_of_data <- big_accuracy_dataframe2_standard %>% filter(seed == i)
  part_of_data$seed = i+1
  big_accuracy_dataframe2_standard <- rbind(big_accuracy_dataframe2_standard,part_of_data)
}
```

```{r}
final_accuracies_standard <- rbind(big_accuracy_dataframe_standard,big_accuracy_dataframe2_standard)


p <- ggplot(final_accuracies_standard, aes(x = Metric, y = Accuracy)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(width = 0.2, size = 2, alpha = 0.3, aes(color = Model, group = interaction(Model, Metric))) +
  labs(title = "Model Accuracy Comparison W/ Normalized Values",
       x = "Metric",
       y = "Cumulative Accuracy (%)") +
  theme_minimal() +
  transition_states(seed, transition_length = 2, state_length = 1, wrap = FALSE) +
  enter_fade() +
  exit_fade() +
  labs(subtitle = "Seed: {closest_state}")

# Animate
anim <- animate(p, duration = 10, fps = 20, width = 800, height = 600, renderer = gifski_renderer())

# Save the animation
anim_save("model_accuracy_comparison_cumulative_standard.gif", anim)
```

```{r}
anim
```
