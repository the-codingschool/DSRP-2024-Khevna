---
title: "mlBreastCancerQuarto"
author: "Tony Song"
format: html
editor: visual
---

## Plan

-   Import Dataset

-   Check Data

-   Process Data

-   Split into training and testing

-   Evaluate Models

-   Feature Scaling (PCA)

-   Compress Data

-   Hyper Parameter Tuning

-   Evaluating Models

-   Cross Validation

-   Select Best Model

-   Shiny App

Import Libraries

```{r}
library(psych)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(shiny)
library(shinythemes)
library(tibble)
library(vroom)
library(fastDummies)
library(caTools)
library(class)
library(paletteer)
library(randomForest)
library(shinyjs)
library(ggcorrplot)
library(utils)
library(reshape2)
library(gridExtra)
library(utils)
library(mlbench)
library(caret)
library(ggbiplot)
library(devtools)
library(varImp)
library(e1071)
library(adabag)
library(rpart)
library(gbm)
library(nnet)
library(glmnet)
```

### Import Dataset

```{r}
setwd("C:/Users/Tony/OneDrive/Documents/tonyR/DSRP-2024-Khevna/tonyWork")
data <- read.csv("../data/breast_cancer_classification_data.csv")
```

### Clean Data

```{r}
data <- data[,c(-1,-33)]
```

```{r}
head(data)
```

```{r}
correlation_matrix <- cor(data[,-1],use = "complete.obs")
```

```{r}
highlyCorrelated <- findCorrelation(cor(data[,-1]), cutoff=0.9,names=TRUE)
highlyCorrelated


```

```{r}
highlyCorrelated <- findCorrelation(cor(select(data,-c("diagnosis","concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean"))), cutoff=0.9,names=TRUE)
highlyCorrelated

```

```{r}
set.seed(42)
dataset <- data
split <- sample.split(dataset$diagnosis,SplitRatio=0.7)
training <- subset(dataset,split==TRUE)
testing <- subset(dataset,split==FALSE)
training$diagnosis <- as.factor(training$diagnosis)
testing$diagnosis <- as.factor(testing$diagnosis)
```

```{r}
trainingSubset <- training %>% select(-c("concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean"))
testingSubset <- testing %>% select(-c("concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean"))
```

```{r}
rf_model <- randomForest(x = training[,-1],
                         y = training$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testing[,-1])

actuals <- testing$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

```{r}
importance_scores <- importance(rf_model)
importance_df <- as.data.frame(importance_scores)
importance_df$Feature <- rownames(importance_df)
importance_sorted <- importance_df[order(-importance_df$MeanDecreaseGini), ]
print(importance_sorted)
varImpPlot(rf_model)
```

```{r}
trainingFeatures <- training %>% select(c("diagnosis","radius_worst","concave.points_worst","perimeter_worst","concave.points_mean","area_worst","radius_mean"))
testingFeatures <- testing %>% select(c("diagnosis","radius_worst","concave.points_worst","perimeter_worst","concave.points_mean","area_worst","radius_mean"))
```

```{r}
rf_model <- randomForest(x = trainingFeatures[,-1],
                         y = trainingFeatures$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testingFeatures[,-1])

actuals <- testingFeatures$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

```{r}
rf_model <- randomForest(x = trainingSubset[,-1],
                         y = trainingSubset$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testingSubset[,-1])

actuals <- testingSubset$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

```{r}
importance_scores <- importance(rf_model)
importance_df <- as.data.frame(importance_scores)
importance_df$Feature <- rownames(importance_df)
importance_sorted <- importance_df[order(-importance_df$MeanDecreaseGini), ]
print(importance_sorted)
varImpPlot(rf_model)
```

```{r}
trainingSubsetFeatures <- trainingSubset %>% select(c("diagnosis","concave.points_worst","radius_worst","concavity_worst","radius_se","compactness_mean"))
testingSubsetFeatures <- testingSubset %>% select(c("diagnosis","concave.points_worst","radius_worst","concavity_worst","radius_se","compactness_mean"))
```

```{r}
rf_model <- randomForest(x = trainingSubsetFeatures[,-1],
                         y = trainingSubsetFeatures$diagnosis, ntree=50)

predictions <- predict(rf_model, newdata = testingSubsetFeatures[,-1])

actuals <- testingSubsetFeatures$diagnosis

confusion_matrix <- table(Predicted = predictions, Actual = actuals)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

```{r}
set.seed(42)
knn_model <- knn(train = training[,-1], test=testing[,-1],cl=training$diagnosis, k=5)

actuals <- testing$diagnosis

confusion_matrix <- table(Predicted = knn_model, Actual = actuals)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

```{r}
set.seed(42)
knn_model <- knn(train = trainingSubset[,-1], test=testingSubset[,-1],cl=training$diagnosis, k=5)

actuals <- testingSubset$diagnosis

confusion_matrix <- table(Predicted = knn_model, Actual = actuals)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

```{r}
p <- cor(select(data,-c("diagnosis","concavity_mean","concave.points_mean","perimeter_worst","radius_mean","perimeter_mean","perimeter_se","area_worst","area_mean","area_se","texture_mean")),use="complete.obs") >0.9
ggcorrplot(p)
```

SVM Model

```{r}
svm_model <- svm(diagnosis ~ ., data = training, kernel = "radial", cost = 1, scale = TRUE)

predictions <- predict(svm_model, testing)
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

```{r}
svm_model <- svm(diagnosis ~ ., data = trainingSubset, kernel = "radial", cost = 1, scale = TRUE)

predictions <- predict(svm_model, testingSubset)
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

Decision Tree Model

```{r}
dt_model <- rpart(diagnosis ~ ., data = training, method = "class")
predictions <- predict(dt_model, testing, type = "class")
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

```{r}
dt_model <- rpart(diagnosis ~ ., data = trainingSubset, method = "class")
predictions <- predict(dt_model, testingSubset, type = "class")
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

Naive Bayes Classifier

```{r}
nb_model <- naiveBayes(diagnosis ~ ., data = training)
predictions <- predict(nb_model, testing)
confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

```{r}
nb_model <- naiveBayes(diagnosis ~ ., data = trainingSubset)
predictions <- predict(nb_model, testingSubset)
confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

Logistic Regression

```{r}
lr_model <- glm(diagnosis ~ ., data = training, family = binomial)
predictions_prob <- predict(lr_model, testing, type = "response")
predictions <- ifelse(predictions_prob > 0.5, "M", "B")
predictions <- factor(predictions, levels = levels(testing$diagnosis))

confusion_matrix <- table(Predicted = predictions, Actual = testing$diagnosis)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

```{r}
lr_model <- glm(diagnosis ~ ., data = trainingSubset, family = binomial)
predictions_prob <- predict(lr_model, testingSubset, type = "response")
predictions <- ifelse(predictions_prob > 0.5, "M", "B")
predictions <- factor(predictions, levels = levels(testingSubset$diagnosis))

confusion_matrix <- table(Predicted = predictions, Actual = testingSubset$diagnosis)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

Adaptive Boosting

```{r}
ab_model <- boosting(diagnosis ~ ., data = training, mfinal = 50)
predictions <- predict(ab_model, newdata = testing)

confusion_matrix <- table(Predicted = predictions$class, Actual = testing$diagnosis)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

```{r}
ab_model <- boosting(diagnosis ~ ., data = trainingSubset, mfinal = 50)
predictions <- predict(ab_model, newdata = testingSubset)

confusion_matrix <- table(Predicted = predictions$class, Actual = testingSubset$diagnosis)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

confusion_matrix
cat("Accuracy: ", round(accuracy*100, 2), "%")
```

Gradient Boosting

```{r}
training1 <- training
testing1 <- testing
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

gb_model <- gbm(
  formula = diagnosis ~ ., 
  data = training1, 
  distribution = "bernoulli", 
  n.trees = 100, 
  interaction.depth = 3, 
  shrinkage = 0.01, 
  n.minobsinnode = 10
)


predictions_prob <- predict(gb_model, newdata = testing1, n.trees = 100, type = "response")
predictions <- ifelse(predictions_prob > 0.5, 1, 0) 

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)


print(confusion_matrix)
cat("Accuracy: ", round(accuracy * 100, 2), "%\n")
```

```{r}
training1 <- trainingSubset
testing1 <- testingSubset
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

gb_model <- gbm(
  formula = diagnosis ~ ., 
  data = training1, 
  distribution = "bernoulli", 
  n.trees = 100, 
  interaction.depth = 3, 
  shrinkage = 0.01, 
  n.minobsinnode = 10
)

predictions_prob <- predict(gb_model, newdata = testing1, n.trees = 100, type = "response")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))


confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

print(confusion_matrix)
cat("Accuracy: ", round(accuracy * 100, 2), "%\n")
```

```{r}
training1 <- training
testing1 <- testing
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

mlp_model <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,       
  decay = 0.01,       
  maxit = 200,        
  linout = FALSE      
)

predictions_prob <- predict(mlp_model, newdata = testing1, type = "raw")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

print(confusion_matrix)
cat("Accuracy: ", round(accuracy * 100, 2), "%\n")
```

```{r}
training1 <- trainingSubset
testing1 <- testingSubset
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

mlp_model <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,       
  decay = 0.01,       
  maxit = 200,        
  linout = FALSE      
)

predictions_prob <- predict(mlp_model, newdata = testing1, type = "raw")
predictions <- ifelse(predictions_prob > 0.5, 1, 0)  

predictions <- factor(predictions, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions, Actual = actual)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

print(confusion_matrix)
cat("Accuracy: ", round(accuracy * 100, 2), "%\n")
```

Neural Classification Chains

```{r}
training1 <- training
testing1 <- testing
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

model1 <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,          
  decay = 0.01,      
  maxit = 200,        
  linout = FALSE      
)

predictions_prob1 <- predict(model1, newdata = testing1, type = "raw")
testing1$pred1 <- ifelse(predictions_prob1 > 0.5, 1, 0)

train_predictions_prob1 <- predict(model1, newdata = training1, type = "raw")
training1$pred1 <- ifelse(train_predictions_prob1 > 0.5, 1, 0)

model2 <- nnet(
  diagnosis ~ . + pred1, 
  data = training1, 
  size = 10,        
  decay = 0.01,       
  maxit = 200,     
  linout = FALSE      
)

predictions_prob2 <- predict(model2, newdata = testing1, type = "raw")
predictions2 <- ifelse(predictions_prob2 > 0.5, 1, 0)

predictions2 <- factor(predictions2, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions2, Actual = actual)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

print(confusion_matrix)
cat("Accuracy: ", round(accuracy * 100, 2), "%\n")
```

```{r}
training1 <- trainingSubset
testing1 <- testingSubset
training1$diagnosis <- as.numeric(training1$diagnosis) - 1
testing1$diagnosis <- as.numeric(testing1$diagnosis) - 1

model1 <- nnet(
  diagnosis ~ ., 
  data = training1, 
  size = 10,          
  decay = 0.01,      
  maxit = 200,        
  linout = FALSE      
)

predictions_prob1 <- predict(model1, newdata = testing1, type = "raw")
testing1$pred1 <- ifelse(predictions_prob1 > 0.5, 1, 0)

train_predictions_prob1 <- predict(model1, newdata = training1, type = "raw")
training1$pred1 <- ifelse(train_predictions_prob1 > 0.5, 1, 0)

model2 <- nnet(
  diagnosis ~ . + pred1, 
  data = training1, 
  size = 10,        
  decay = 0.01,       
  maxit = 200,     
  linout = FALSE      
)

predictions_prob2 <- predict(model2, newdata = testing1, type = "raw")
predictions2 <- ifelse(predictions_prob2 > 0.5, 1, 0)

predictions2 <- factor(predictions2, levels = c(0, 1))
actual <- factor(testing1$diagnosis, levels = c(0, 1))

confusion_matrix <- table(Predicted = predictions2, Actual = actual)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

print(confusion_matrix)
cat("Accuracy: ", round(accuracy * 100, 2), "%\n")
```
